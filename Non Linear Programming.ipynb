{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Non Linear Programming (NLP)\n",
    "In mathematics, nonlinear programming (NLP) is the process of solving an optimization problem where some of the constraints and/or the objective function are nonlinear. Some non-linear functions can be convex.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Formally a nonlinear minimization problem is an optimization problem of the form\n",
    "\n",
    "$${\\displaystyle {\\begin{aligned}{\\text{minimize }}&f(\\vec{x})\\\\{\\text{subject to }}&g_{i}(\\vec{x})\\leq 0{\\text{ for each }}i\\in \\{1,\\dotsc ,m\\}\\\\&h_{j}(\\vec{x})=0{\\text{ for each }}j\\in \\{1,\\dotsc ,p\\}\\end{aligned}}}$$\n",
    "\n",
    "where:\n",
    "- n, m, and p be positive integers.\n",
    "\n",
    "- $\\vec{x} \\in \\mathbb{R}^n$.\n",
    "\n",
    "- f is the objective function.\n",
    "\n",
    "- g and h are constraint functions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notation:\n",
    "    - feasible region: the region described by the constraints."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2-dimensional example\n",
    "\n",
    "The objective function to be MAXIMIZED is this: \n",
    "$$f(\\vec{x}) = x1 + x2$$  \n",
    "\n",
    "It's a plane. The general equation of a plane is ax+by+cz+d=0. In our case we have z=x+y. The vector normal to the plane is given by (a,b,c), so in our case (1,1,1).\n",
    "\n",
    "The constraints are these:\n",
    "\n",
    "$x_1 ≥ 0$\n",
    "\n",
    "$x_2 ≥ 0$\n",
    "\n",
    "$x_1^2 + x_2^2 ≥ 1 $\n",
    "\n",
    "$x_1^2 + x_2^2 ≤ 2$\n",
    "\n",
    "Note:\n",
    "- the first two constraints tell us that the solution must be found in the first quadrant of the x1 , x2 plane.\n",
    "\n",
    "- the last two constraints tell us that the solution must be found in a circolar crown with internal radious = 1 and external radious = $\\sqrt2$.\n",
    "\n",
    "Solution: If you think about it is something in between with the maximum x1 value and the maximum x2 value which are feasibles. So the solution can be showed with an image by using a line: the solution is the point of tangency between the line and the circolar crown:\n",
    "\n",
    "<img src= \"2_nlp_1.png\">\n",
    "(x = x1, y= x2).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How are  UNCONSTRAINED NLP solved? ITERATIVE METHODS, OR HEURISTICS.\n",
    "Let's talk about iterative methods to solve NLP.\n",
    "\n",
    "- Iterative methods: In particular for as regard optimization we are interested in \"DESCENT METHODS\" which are based on this idea:\n",
    "  We start from a starting point $\\vec{x}_o$ and we want at each iteration find another x in which the function has a lower     value. So at each step we want $f(\\vec{x}_{k+1}) < f(\\vec{x}_k)$. In general the descent methods do this at each iteration:\n",
    "  $$ \\vec{x}_{k+1} = \\vec{x}_k + \\alpha_k \\vec{\\rho}_k $$\n",
    "  where $\\alpha$ is the \"step length. Instead $\\rho$ is the \"descent direction\" vector, which has the same dimension of $\\vec{x}$. We'll discuss the main methods later on, for the moment let's classify them. They differ on the way $\\alpha$ and $\\rho$ are computed. \n",
    "  \n",
    "    about $\\alpha$:\n",
    "\n",
    "  - constant\n",
    "  - EXACT LINE SEARCH methods: at each step is sought the step length which minimizes the function along the given direction. They have an higher cost of course.\n",
    "  - INEXACT LINE SEARCH (= \"BRACKTRACKING\"): at each iteration step an there's an additional iterative procedure which stops when the value of alpha satisfies two properties: 1)  $f(\\vec{x}_{k+1}) < f(\\vec{x}_k)$ 2) a convergence test.\n",
    "  \n",
    "  about $\\rho$:\n",
    "      \n",
    "  - FIRST ORDER METHODS: gradient methods: evaluate gradients, or approximate gradients in some way (or even subgradients)\n",
    "    usually each step is faster, but it could converge in much more time (Q-LINEAR SPEED):\n",
    "  \n",
    "      - Gradient descent (alternatively, \"steepest descent\" or \"steepest ascent\"): A (slow) method of historical and theoretical interest, which has had renewed interest for finding approximate solutions of enormous problems.  \n",
    "      - Quasi-Newton methods: Iterative methods for medium-large problems (e.g. N<1000).\n",
    "      - Gauss-Newton and Levenberg-Marquardt, they are modifications of the Newton's method in order to do not compute the Hessian matrix! But they can be used only to minimize a sum of squared function values (indeed are mostly used for non linear square problems).\n",
    "      - ... \n",
    "  - SECOND ORDER METHODS: Newton like methods: evaluate Hessians (or approximate Hessians, using finite differences)\n",
    "    Usually each step is more costly, but it has an higher convergence speed (Q-QUADRATIC SPEED):\n",
    "\n",
    "    - Newton's method \n",
    "    - ...\n",
    "    \n",
    "\n",
    "SEE THOSE METHODS IN DETAIL IN THEIR SPECIFIC NOTEBOOK."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HOW IS CONSTRAINED NLP SOLVED? WITH LAGRANGIAN METHOD.\n",
    "See the method description on its relative notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
